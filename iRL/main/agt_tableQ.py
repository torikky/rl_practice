"""
agt_tableQ.py
Qテーブルを使ったQ学習アルゴリズム
"""
import sys
import pickle
import numpy as np

# 自作モジュール
import core


class TableQAgt(core.coreAgt):
    """Qテーブルを使ったQ学習エージェントクラス"""

    def __init__(  # 引数とデフォルト値の設定 (A)
        self,
        n_act=2,  # int: 行動の種類数
        init_val_Q=0,  # float: Q値の初期値
        epsilon=0.1,  # float: 乱雑度
        alpha=0.1,  # float: 学習率
        gamma=0.9,  # float: 割引率
        max_memory=500,  # int: 記憶する最大の観測数
        filepath=None,  # str: 保存用ファイル名
    ):
        """初期処理"""
        # 引数の設定は適時編集
        self.epsilon = epsilon
        # ------------------------- 編集ここから
        self.n_act = n_act
        # エージェントのハイパーパラメータ (B)
        self.init_val_Q = init_val_Q
        self.gamma = gamma
        self.alpha = alpha

        # 保存ファイル名 (C)
        self.filepath = filepath

        # Qテーブル関連 (D)
        self.Q = {}  # Qテーブル
        self.len_Q = 0  # Qテーブルに登録した観測の数
        self.max_memory = max_memory
        # ------------------------- ここまで

    def select_action(self, obs):
        """観測に対して行動を出力"""
        # ------------------------- 編集ここから
        # obsを文字列に変換 (A)
        obs = str(obs)

        # obs が登録されていなかったら初期値を与えて登録 (B)
        self._check_and_add_observation(obs)

        # 確率的に処理を分岐 (C)
        if np.random.rand() < self.epsilon:
            # epsilon の確率(D)
            act = np.random.randint(0, self.n_act)  # ランダム行動
        else:
            # 1-epsilon の確率(E)
            act = np.argmax(self.Q[obs])  # Qを最大にする行動
        # ------------------------- ここまで
        return act

    def _check_and_add_observation(self, obs):
        """obs が登録されていなかったら初期値を与えて登録"""
        if obs not in self.Q:  # (A)
            self.Q[obs] = [self.init_val_Q] * self.n_act  # (B)
            self.len_Q += 1  # (C)
            if self.len_Q > self.max_memory:  # (D)
                print(f"観測の登録数が上限 " + f"{self.max_memory:d} に達しました。")
                sys.exit()
            if (self.len_Q < 100 and self.len_Q % 10 == 0) or (self.len_Q % 100 == 0):  # (E)
                print(f"the number of obs in Q-table" + f" --- {self.len_Q:d}")

    def learn(self, obs, act, rwd, done, next_obs):
        """学習"""
        if rwd is None:  # rwdがNoneだったら戻る(A)
            return
        # ------------------------- 編集ここから
        # obs, next_obs を文字列に変換 (B)
        obs = str(obs)
        next_obs = str(next_obs)

        # next_obs が登録されていなかったら初期値を与えて登録 (C)
        self._check_and_add_observation(next_obs)

        # 学習のターゲットを作成 (D)
        if done is True:
            target = rwd
        else:
            target = rwd + self.gamma * max(self.Q[next_obs])

        # Qをターゲットに近づける (E)
        self.Q[obs][act] = (1 - self.alpha) * self.Q[obs][act] + self.alpha * target
        # ------------------------- ここまで

    def get_Q(self, obs):
        """観測に対するQ値を出力"""
        # ------------------------- 編集ここから
        obs = str(obs)
        if obs in self.Q:  # obsがQにある (A)
            val = self.Q[obs]
            Q = np.array(val)
        else:  # obsがQにない (B)
            Q = None
        # ------------------------- ここまで
        return Q

    def save_weights(self, filepath=None):
        """方策のパラメータの保存"""
        # ------------------------- 編集ここから
        # Qテーブルの保存
        if filepath is None:
            filepath = self.filepath + ".pkl"
        with open(filepath, mode="wb") as f:
            pickle.dump(self.Q, f)
        # ------------------------- ここまで

    def load_weights(self, filepath=None):
        """方策のパラメータの読み込み"""
        # ------------------------- 編集ここから
        # Qテーブルの読み込み
        if filepath is None:
            filepath = self.filepath + ".pkl"
        with open(filepath, mode="rb") as f:
            self.Q = pickle.load(f)
        # ------------------------- ここまで


if __name__ == "__main__":
    # 学習のステップ数 (A)
    n_step = 5000

    # コマンドライン引数 (B)
    argvs = sys.argv
    if len(argvs) > 1:
        n_step = int(argvs[1])
    print(f"{n_step:d}ステップの学習シミュレーション開始")

    # 環境の準備 (C)
    from env_corridor import CorridorEnv

    env = CorridorEnv()

    # 環境のパラメータの与え方の例
    """
    env = CorridorEnv(
        field_length=6,
        crystal_candidate=(2, 3, 4, 5),
        rwd_fail=-1,
        rwd_move=0,
        rwd_crystal=5,
    )
    """

    # エージェントの準備 (D)
    agt = TableQAgt(
        alpha=0.2,
        gamma=1,
        epsilon=0.5,
    )

    # 学習シミュレーション (E)
    obs = env.reset()
    for t in range(n_step):
        # エージェントが行動を選ぶ (F)
        act = agt.select_action(obs)

        # 環境が報酬と次の観測を決める (G)
        rwd, done, next_obs = env.step(act)

        # エージェントが学習する (H)
        agt.learn(obs, act, rwd, done, next_obs)

        # next_obsを次の学習のために保持 (I)
        obs = next_obs

    # 学習後のQ値の表示のための入力観測 (J)
    obss = [
        "[1 0 2 0]",
        "[0 1 2 0]",
        "[0 0 1 0]",
        "[0 0 2 1]",
        "[1 0 0 2]",
        "[0 1 0 2]",
        "[0 0 1 2]",
        "[0 0 0 1]",
    ]

    # 学習後のQ値の表示 (K)
    print("")
    print("学習後のQ値")
    for obs in obss:
        q_vals = agt.get_Q(obs)
        if q_vals is not None:
            msg = f"{obs}: " + f"{agt.Q[obs][0]: .2f}, " + f"{agt.Q[obs][1]: .2f}"
            print(msg)
        else:
            print(f"{obs}:")

    # 学習結果を見せるためのシミュレーションの準備(L)
    import cv2

    agt.epsilon = 0

    # 強化学習情報の初期化 (M)
    t = 0
    obs = env.reset()
    act = None
    rwd = None
    done = None
    next_obs = None

    # 開始メッセージを表示 (N)
    print("")
    print("学習なしシミュレーション開始")

    # 強化学習情報表示関数の定義 (O)
    def show_info(t, act, rwd, done, obs, isFirst=None):
        if rwd is None:
            if isFirst:
                tt = t
            else:
                tt = t + 1
            print("")
            print(f"x({tt:d})={str(obs):s}")
        else:
            msg = (
                f"a({t:d})={act:d}, "
                + f"r({t:d})={rwd: .2f}, "
                + f"done({t:d})={done:}, "
                + f"x({t + 1:d})={str(obs):s}"
            )
            print(msg)

    # 強化学習情報表示 (P)
    show_info(t, act, rwd, done, obs, isFirst=True)

    # 学習なしシミュレーション (Q)
    while True:
        # 画面表示 (R)
        image = env.render()
        cv2.imshow("agt", image)

        # キーの受付と終了処理 (S)
        key = cv2.waitKey(0)
        if key == ord("q"):
            break

        # エージェントの行動選択 (T)
        act = agt.select_action(obs)

        # 環境の更新 (U)
        rwd, done, obs = env.step(act)

        # 強化学習情報表示 (V)
        show_info(t, act, rwd, done, obs)
        t += 1
